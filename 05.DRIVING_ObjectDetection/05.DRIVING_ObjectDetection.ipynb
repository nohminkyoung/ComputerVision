{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. 데이터셋 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/DRIVING-DATASET/Detection/'\n",
    "data_df = pd.read_csv(os.path.join(data_dir, 'df.csv'))\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 한개 살펴보기\n",
    "index = 859\n",
    "img_files = [fn for fn in os.listdir(os.path.join(data_dir,'images')) if fn.endswith('jpg')] #이미지들 리스트\n",
    "img_file = img_files[index]\n",
    "img_path = os.path.join(data_dir,'images' ,img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(img_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바운딩 박스 그리기\n",
    "\n",
    "BOX_COLOR = {'Bus':(200, 0, 0), 'Truck':(0, 0, 200)}\n",
    "CLASS_ID_TO_NAME = {0: 'Bus', 1: 'Truck'}\n",
    "\n",
    "def visualize(image, bounding_boxes,class_ids,color=BOX_COLOR, thickness=2):\n",
    "    \n",
    "    for class_id, bounding_box in zip(class_ids,bounding_boxes): \n",
    "        class_name = CLASS_ID_TO_NAME[class_id]\n",
    "        x_min, y_min, x_max, y_max = map(int,bounding_box) #정수의 값을 넣어야 그려짐\n",
    "\n",
    "        \n",
    "    \n",
    "        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color=color[class_name], thickness=thickness)\n",
    "        \n",
    "        ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "        cv2.rectangle(image, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), color[class_name], -1)\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            text=class_name,\n",
    "            org=(x_min, y_min - int(0.3 * text_height)),\n",
    "            fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=0.35, \n",
    "            color=(255,255,255), \n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAME_TO_ID = {'Bus': 0, 'Truck': 1} # 버스, 트럭의 키값을 가진 딕셔너리 -> string의 classid를 수치화하기위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 그리기 위한 데이터\n",
    "img_id = img_file.split('.')[0]\n",
    "img_df = data_df[data_df['ImageID']==img_id]\n",
    "class_names = img_df['LabelName'].values\n",
    "class_ids = np.array([CLASS_NAME_TO_ID[class_name] for class_name in class_names])\n",
    "bounding_box = img_df[[\"XMin\", \"XMax\", \"YMin\", \"YMax\"]].values\n",
    "bounding_box[:, [1,2]] = bounding_box[:, [2,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 크기에 따라서 노멀라이즈 되어있음, 이미지 크기를 곱해서 반환해서 사용해야함\n",
    "bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_h, img_w, _ = img.shape\n",
    "\n",
    "class_id = CLASS_NAME_TO_ID[class_names[0]]\n",
    "\n",
    "# 노멀라이즈 되어있는 바운딩 박스의 좌표를 복원\n",
    "\n",
    "bounding_box[:,[0,2]] *= img_w\n",
    "bounding_box[:,[1,3]] *= img_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "canvas = visualize(img, bounding_box, class_ids)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(canvas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스별 이미지 확인\n",
    "from ipywidgets import interact\n",
    "img_files = [fn for fn in os.listdir(os.path.join(data_dir,'images')) if fn.endswith('jpg')] #이미지들 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(index=(0, len(img_files)-1))\n",
    "def show_imgbox(index=1):\n",
    "    img_file = img_files[index]\n",
    "    img_path = os.path.join(data_dir, 'images',img_file)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "\n",
    "    img_id = img_file.split('.')[0]\n",
    "    img_df = data_df[data_df['ImageID']==img_id]\n",
    "    bounding_box = img_df[['XMin','XMax','YMin','YMax']].values\n",
    "    bounding_box[:, [1,2]] = bounding_box[:, [2,1]]\n",
    "\n",
    "    img_h, img_w, _ = img.shape\n",
    "    bounding_box[:,[0,2]] *= img_w\n",
    "    bounding_box[:,[1,3]] *= img_h   \n",
    "    \n",
    "    class_name = img_df['LabelName'].values\n",
    "    class_ids = np.array([CLASS_NAME_TO_ID[class_name] for class_name in class_names])\n",
    "\n",
    "    canvas = visualize(img, bounding_box, class_ids)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(canvas)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# 2. 데이터 셋 및 데이터로더 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Dataset():\n",
    "    def __init__(self, data_dir, phase, transformer=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.phase = phase\n",
    "        self.transformer = transformer\n",
    "\n",
    "        self.data_df = pd.read_csv(os.path.join(self.data_dir, 'df.csv'))\n",
    "        self.img_files = [fn for fn in os.listdir(os.path.join(self.data_dir,phase)) if fn.endswith('jpg')]\n",
    "\n",
    "        self.CLASS_NAME_TO_ID = {'Bus': 0, 'Truck': 1}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #input = img\n",
    "        #target = label(box, class_id)\n",
    "        #get_image로 filename을 가지고오고 이것을 이용해 get_label로 box와 classid를 가져옴\n",
    "\n",
    "        img, filename = self.get_img(index)\n",
    "        boxes, class_ids = self.get_label(filename)\n",
    "\n",
    "        img_h, img_w,_ = img.shape\n",
    "\n",
    "        if self.transformer :\n",
    "            img = self.transformer(img)\n",
    "            _, img_h, img_w = img.shape #transformer에서 imgresize가 되었을수도있음\n",
    "\n",
    "        boxes[:,[0,2]] *= img_w\n",
    "        boxes[:,[1,3]] *= img_h\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = torch.Tensor(boxes).float() #regression을 수행할것이라 float\n",
    "        target[\"labels\"] = torch.Tensor(class_ids).long() #int형\n",
    "\n",
    "        return img, target, filename\n",
    "        \n",
    "\n",
    "    #getitem의 기능을 보충하기 위해서 생성\n",
    "    def get_img(self, index):\n",
    "        filename = self.img_files[index]\n",
    "\n",
    "        img_path = os.path.join(self.data_dir,self.phase,filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        return img, filename\n",
    "\n",
    "    def get_label(self, filename):\n",
    "        img_id = filename.split('.')[0]\n",
    "        img_df = self.data_df[self.data_df['ImageID']==img_id]\n",
    "        boxes = img_df[['XMin','XMax','YMin','YMax']].values\n",
    "        boxes[:, [1,2]] = boxes[:, [2,1]] # model [x_min, y_min, x_max, y_max] 포멧으로 받음\n",
    "\n",
    "        class_names = img_df['LabelName'].values\n",
    "        class_ids = np.array([self.CLASS_NAME_TO_ID[class_name] for class_name in class_names])\n",
    "\n",
    "        return boxes,class_ids\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/DRIVING-DATASET/Detection/'\n",
    "dataset = My_Dataset(data_dir, 'train')\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 5\n",
    "image, target, filename = dataset[index]\n",
    "boxes = target['boxes'].numpy()\n",
    "class_ids = target['labels'].numpy()\n",
    "\n",
    "canvas = visualize(image, boxes, class_ids)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(canvas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 448\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size = (IMG_SIZE,IMG_SIZE)),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    img_list = []\n",
    "    target_list = []\n",
    "    filename_list = []\n",
    "\n",
    "    for a,b,c in batch:\n",
    "        img_list.append(a)\n",
    "        target_list.append(b)\n",
    "        filename_list.append(c)\n",
    "\n",
    "    return img_list, target_list, filename_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/DRIVING-DATASET/Detection/'\n",
    "BATCH_SIZE = 6\n",
    "\n",
    "trainset = My_Dataset(data_dir=data_dir, phase=\"train\", transformer=transformer)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, batch in enumerate(trainloader):\n",
    "    images = batch[0]\n",
    "    targets = batch[1]\n",
    "    filenames = batch[2]\n",
    "\n",
    "    print(len(images),len(targets), len(filenames))\n",
    "    \n",
    "    if index == 0:\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def My_DataLoader(data_dir,transfomer, batch_size=4):\n",
    "\n",
    "    data_loaders = {}\n",
    "\n",
    "    train_dataset = My_Dataset(data_dir, 'train', transformer)\n",
    "    data_loaders['train'] = DataLoader(train_dataset, batch_size = batch_size, shuffle=True, collate_fn = collate_fn)\n",
    "\n",
    "    val_dataset = My_Dataset(data_dir, 'val', transformer)\n",
    "    data_loaders['val'] = DataLoader(val_dataset, batch_size = 1, shuffle=False, collate_fn = collate_fn)\n",
    "\n",
    "    return data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/DRIVING-DATASET/Detection/'\n",
    "dloaders = My_DataLoader(data_dir,transformer,batch_size=4)\n",
    "\n",
    "for phase in [\"train\", \"val\"]:\n",
    "    for index, batch in enumerate(dloaders[phase]):\n",
    "        img = batch[0]\n",
    "        targets = batch[1]\n",
    "        filenames = batch[2]\n",
    "        print(len(img),len(targets), len(filenames))\n",
    "        \n",
    "        if index == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# 3. Faster RCNN 모델 불러와서 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor #모델 변경할 떄 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True) #ResNet-50 아키텍처와 Feature Pyramid Network기반\n",
    "model\n",
    "#box_predictor 의 cls_score - class의 수, bbox_pred - class수 *4(사각형 꼭지점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = model.roi_heads.box_predictor.cls_score.in_features # 기존의 input값\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2) #2 : 2개의 클래스로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#변경 확인\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_class):\n",
    "    model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_class)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "# 4. 학습코드 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델이 train모드일 때 list형태의 img와 target을 받으면 loss를 반환해줌(FasterRCNN내부에 손실 계산 기능이 있음)\n",
    " #eval모드일 때에는 image만 input으로 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'train'\n",
    "model.train()\n",
    "\n",
    "for index, batch in enumerate(dloaders[phase]):\n",
    "    imgs = batch[0]\n",
    "    targets = batch[1]\n",
    "    filenames = batch[2]\n",
    "\n",
    "    # 이미지, 타겟을 리스트로 (굳이x)\n",
    "    imgs = list(img for img in imgs)\n",
    "    targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "    loss = model(imgs, targets)\n",
    "\n",
    "    if index == 0:\n",
    "        break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#첫번째 스테이지단계에서 수행하는 CNN에 의한 loss\n",
    "#loss_objectness : 어떤 대상에 대해서 분류를 하지 않고, regionproposal구간에 오브젝트가 있는지 없는지를 구분하는 loss\n",
    "#loss_rpn_box_reg : Region Proposal Network에서 발견된 객체 후보들의 경계 상자regression box의 예측값과 Target 간의 차이를 측정\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model이 train모드일때는 이미지와 타겟을 받아서 loss를 계산해줌, eval모드일 때는 이미지만 받아서 예측값을 반환해줌\n",
    "# 아래 train_one_epoch에서는 모델을 val부분에서까지 train모드로 사용 - loss 계산을 위함\n",
    "# 하지만 with torch.set_grad_enabled(phase == \"train\"): 부분과 if phase == \"train\":에서만 backward와 grad를 계산했기 때문에 train모드로 val을 진행해도\n",
    "# grad가 갱신되지 않아 학습에 영향을 미치지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict #매 에폭마다 loss를 담기 위해서 선언(존재하지 않는 키(key)에 접근하려고 할 때 에러가 발생하지 않고, 기본값을 반환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloaders, model, optimizer, device):#모델이 직접 loss를 구해서 criterion이 필요없음\n",
    "\n",
    "    train_loss = defaultdict(float)\n",
    "    val_loss = defaultdict(float)\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        for index, batch in enumerate(dataloaders[phase]):\n",
    "            imgs = batch[0]\n",
    "            targets = batch[1]\n",
    "            filename = batch[2]\n",
    "\n",
    "            imgs = [img.to(device) for img in imgs]\n",
    "            targets = [{k:v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            with torch.set_grad_enabled(phase=='train'): #train일 경우에만 loss로 grad를 구함\n",
    "                losses = model(imgs, targets)\n",
    "                total_loss = sum(loss for loss in losses.values())\n",
    "\n",
    "                if phase == 'train' :\n",
    "                    optimizer.zero_grad()\n",
    "                    total_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    if (index>0) and (index%VERBOSE_FREQ == 0) : \n",
    "                        text = f'{index}/{len(dataloaders[phase])} -'\n",
    "                        for k,v in losses.items():\n",
    "                            text +=f'{k} : {v.item():.4f}'\n",
    "                        print(text)\n",
    "\n",
    "                        for k,v in losses.items():\n",
    "                            train_loss[k] += v.item()\n",
    "                        train_loss['total_loss'] += total_loss.item() #backward되는 부분이 loss에 포함되어있어서 item을 붙여줘야함\n",
    "\n",
    "                else : \n",
    "                    for k, v in losses.items():\n",
    "                        val_loss[k] += v.item()\n",
    "                    val_loss['total_loss'] += total_loss.item()\n",
    "\n",
    "    for k in train_loss.key():\n",
    "        train_loss[k] /= len(dataloaders['train'])\n",
    "        val_loss[k] /= len(dataloaders['val'])\n",
    "\n",
    "    return train_loss, val_loss         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/DRIVING-DATASET/Detection/'\n",
    "is_cuda = True\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "IMAGE_SIZE = 448\n",
    "BATCH_SIZE = 6\n",
    "VERBOSE_FREQ = 30\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataloaders = My_DataLoader(data_dir,transformer,batch_size=4)\n",
    "model = build_model(NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_state, model_name, save_dir='./trained_model'):\n",
    "    os.mkdir(save_dir, exist_ok=True)\n",
    "    torch.save(model_state, os.path.join(save_dir,model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, val_loss = train_one_epoch(dataloaders, model, optimizer, DEVICE)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"epoch:{epoch+1}/{num_epochs} - Train Loss: {train_loss['total_loss']:.4f}, Val Loss: {val_loss['total_loss']:.4f}\")\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        save_model(model.state_dict(), f'model_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss_classifier = [] \n",
    "tr_loss_box_reg = []\n",
    "tr_loss_objectness = []\n",
    "tr_loss_rpn_box_reg = []\n",
    "tr_loss_total = []\n",
    "\n",
    "for tr_loss in train_losses:\n",
    "    tr_loss_classifier.append(tr_loss['loss_classifier'])\n",
    "    tr_loss_box_reg.append(tr_loss['loss_box_reg'])\n",
    "    tr_loss_objectness.append(tr_loss['loss_objectness'])\n",
    "    tr_loss_rpn_box_reg.append(tr_loss['loss_rpn_box_reg'])\n",
    "    tr_loss_total.append(tr_loss['total_loss'])\n",
    "\n",
    "val_loss_classifier = [] \n",
    "val_loss_box_reg = []\n",
    "val_loss_objectness = []\n",
    "val_loss_rpn_box_reg = []\n",
    "val_loss_total = []\n",
    "\n",
    "for vl_loss in val_losses:\n",
    "    val_loss_classifier.append(vl_loss['loss_classifier'])\n",
    "    val_loss_box_reg.append(vl_loss['loss_box_reg'])\n",
    "    val_loss_objectness.append(vl_loss['loss_objectness'])\n",
    "    val_loss_rpn_box_reg.append(vl_loss['loss_rpn_box_reg'])\n",
    "    val_loss_total.append(vl_loss['total_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(tr_loss_total, label=\"train_total_loss\")\n",
    "plt.plot(tr_loss_classifier, label=\"train_loss_classifier\")\n",
    "plt.plot(tr_loss_box_reg,  label=\"train_loss_box_reg\")\n",
    "plt.plot(tr_loss_objectness, label=\"train_loss_objectness\")\n",
    "plt.plot(tr_loss_rpn_box_reg,  label=\"train_loss_rpn_box_reg\")\n",
    "\n",
    "plt.plot(val_loss_total, label=\"train_total_loss\")\n",
    "plt.plot(val_loss_classifier, label=\"val_loss_classifier\")\n",
    "plt.plot(val_loss_box_reg,  label=\"val_loss_box_reg\")\n",
    "plt.plot(val_loss_objectness, label=\"val_loss_objectness\")\n",
    "plt.plot(val_loss_rpn_box_reg,  label=\"val_loss_rpn_box_reg\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.grid(\"on\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "# 5. Confidence threshold 와 Non-maximum suppression(NMS)적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 불러오는 함수\n",
    "def load_model(ckpt_path, num_class, device):\n",
    "    \n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    model = build_model(num_class=num_class)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 불러오기\n",
    "NUM_CLASSES = 2\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data_dir = '../../data/DRIVING-DATASET/Detection/'\n",
    "\n",
    "model = load_model(ckpt_path='./trained_model/model_30.pth', num_class=NUM_CLASSES,device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1개의 불러온 모델의 예측값에 대해 확인해보기 \n",
    "for index, batch in enumerate(dataloaders['val']):\n",
    "    imgs = batch[0]\n",
    "    imgs = list(img.to(DEVICE) for img in imgs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(imgs)\n",
    "\n",
    "    if index==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 후처리함수\n",
    "def postprocess(prediction, conf_threshold=0.2, IoU_threshold=0.1):\n",
    "    pred_box = prediction['boxes'].cpu().detach() #detach() : Tensor의 그라디언트 계산을 멈추는 역할\n",
    "    pred_label = prediction['labels'].cpu().detach()\n",
    "    pred_score = prediction['scores'].cpu().detach()\n",
    "\n",
    "    #conf_threchold\n",
    "    valid_index_conf = pred_score>conf_threshold\n",
    "    pred_box = pred_box[valid_index_conf]\n",
    "    pred_label = pred_label[valid_index_conf]\n",
    "    pred_score = pred_score[valid_index_conf]\n",
    "\n",
    "    #nms_threshold\n",
    "    #nms : box, score, IoU_threshold를 받음,\n",
    "    #box좌표 : x1, y1, x2, y2로 들어감(FasterRCNN은 xmin,ymin, xmax,ymax의 순으로 박스를 반환해줘서 그대로 사용 가능)\n",
    "    valid_index_nms = nms(pred_box, pred_score, IoU_threshold)\n",
    "    pred_box = pred_box[valid_index_nms].numpy() # 이미지 그리기 위해서 numpy로 변환\n",
    "    pred_label = pred_label[valid_index_nms].numpy()\n",
    "    pred_score = pred_score[valid_index_nms].numpy()\n",
    "\n",
    "\n",
    "    ##np.newaxis : 배열의 차원을 하나 늘려주는 역할 -> pred_box에 맞춰주기 위해서 나머지를 한차원씩 늘려줌\n",
    "    return np.concatenate((pred_box,pred_score[:, np.newaxis], pred_label[:, np.newaxis]), axis=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.newaxis 예시\n",
    "        # # 1차원 배열\n",
    "        # arr_1d = np.array([1, 2, 3])\n",
    "        \n",
    "        # # 1차원 배열을 2차원으로 확장\n",
    "        # arr_2d = arr_1d[:, np.newaxis]\n",
    "        \n",
    "        # print(arr_2d.shape)  # 출력: (3, 1)\n",
    "        \n",
    "        # # 2차원 배열\n",
    "        # arr_2d = np.array([[1, 2, 3]])\n",
    "        \n",
    "        # # 2차원 배열을 3차원으로 확장\n",
    "        # arr_3d = arr_2d[:, :, np.newaxis]\n",
    "        \n",
    "        # print(arr_3d.shape)  # 출력: (1, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictrion 확인\n",
    "for index, batch in enumerate(dataloaders['val']):\n",
    "    imges = batch[0]\n",
    "    img = [img.to(DEVICE)for img in imges]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(img)\n",
    "\n",
    "    prediction = postprocess(prediction[0])\n",
    "\n",
    "    if index==0:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction # box, score, label의 순서로 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측하기\n",
    "pred_imgs = []\n",
    "pred_labels = []\n",
    "\n",
    "for index, batch in enumerate(dataloaders['val']):\n",
    "    imgs = batch[0]\n",
    "    img = [img.to(DEVICE)for img in imgs]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(img)\n",
    "\n",
    "    prediction = postprocess(prediction[0])\n",
    "\n",
    "    # 예측된 상자의 크기가 이미지의 크기보다 큰것을 방지\n",
    "    # clip : 배열의 요소가 최솟값보다 작으면 최솟값으로, 최댓값보다 크면 최댓값으로 설정\n",
    "    # xmax와 ymax를 제한\n",
    "    prediction[:,2].clip(min = 0, max = imgs[0].shape[1])\n",
    "    prediction[:,3].clip(min = 0, max = imgs[0].shape[0])\n",
    "\n",
    "    pred_imgs.append(imgs[0])\n",
    "    pred_labels.append(prediction)\n",
    "\n",
    "    if index==30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값 이미지로 살펴보기\n",
    "@interact(index=(0, len(pred_imgs)-1))\n",
    "def show_predict(index=0):\n",
    "    int_labels = pred_labels[index][:, 5].astype(int)\n",
    "\n",
    "    # 기존의 예측 이미지가 normalization되어있어서 색감이 이상함, \n",
    "    # make_grid의 normalize를 이용하면 pred_imgs[index]의 내부의 값을 0~1사이의 값으로 재조정\n",
    "    # 이에 255를 곱해줘서 원래의 색감으로 돌아옴\n",
    "    image = make_grid(pred_imgs[index], normalize=True).permute(1,2,0).numpy() \n",
    "    image = (image * 255).astype(np.uint8) #imshow가능하도록\n",
    "    img = image.copy()\n",
    "    \n",
    "    result = visualize(img, pred_labels[index][:, 0:4], int_labels)\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(result)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "# 6. 성능 검증을 위한 지표 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pycocotools.coco import COCO # COCO 데이터셋을 로드하고 사용하기 위한 도구,데이터셋의 이미지, annotation, 클래스 레이블 등을 로드하고 검색\n",
    "from pycocotools.cocoeval import COCOeval #모델의 성능을 평가하기 위한 도구, 모델의 예측 결과를 COCO 데이터셋의 실제 annotation과 비교(정확도, 재현율, mAP 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "annfile = '../../data/DRIVING-DATASET/Detection/val.json'#groundtruth 가 담겨있는 json파일\n",
    "\n",
    "with open(annfile, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "imgToid = json_data['imageToid'] #filename을 이용해서 file의 image_id를 가져오고, 그 id를 이용해 annotation을 불러오기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coco를 이용해 모델을 평가하기 위해서 coco에 annotation정보를 입력해 GT를 생성\n",
    "cocoGT = COCO(annfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#원래는 xmin,ymin,xmax,ymax의 포멧, coco는 xmin,ymin,w,h포멧이기때문에 바꿔줘야함\n",
    "def changeformat(box):\n",
    "    Xmin = box[:,0]\n",
    "    Ymin = box[:,1]\n",
    "    w = box[:,2] - box[:,0]\n",
    "    h = box[:,3] - box[:,1]\n",
    "\n",
    "    return np.stack((Xmin, Ymin, w, h), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 예측 구하기\n",
    "COCO_anno = []\n",
    "\n",
    "for index, batch in enumerate(dataloaders['val']):\n",
    "    imgs = batch[0]\n",
    "    filename = batch[2][0]\n",
    "    \n",
    "    img = list(img.to(DEVICE) for img in imgs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(img)\n",
    "\n",
    "    prediction = postprocess(prediction[0])\n",
    "    prediction[:,2].clip(min=0, max=img[0].shape[1])\n",
    "    prediction[:,3].clip(min=0, max=img[0].shape[0])\n",
    "\n",
    "    #박스포멧 바꿔주기\n",
    "    box_xywh = changeformat(prediction[:, 0:4])\n",
    "    score = prediction[:,4][:,np.newaxis] #[:,np.newaxis]:box포멧에 맞추기 위해\n",
    "    cls_id = prediction[:,5][:,np.newaxis]\n",
    "    img_id = np.array([imgToid[filename]]*len(cls_id))[:,np.newaxis] #* len(cls_id): 하나의 파일에 여러개의 오브젝트가 있을것이기 때문에 곱해서 개수를 맞춰줌\n",
    "\n",
    "    COCO_anno.append(np.concatenate((img_id, box_xywh, score, cls_id), axis=1))\n",
    "\n",
    "    if index %50 ==0:\n",
    "        print(f\"{index}/{len(dataloaders['val'])} Done\")\n",
    "\n",
    "COCO_anno = np.concatenate(COCO_anno, axis = 0) # 하나의 배열로 합쳐줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocoDT = cocoGT.loadRes(COCO_anno) #모델이 예측한 COCO_anno데이터를 cocoGT와 같은 형식으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "annType = \"bbox\" #annotation중 어떤걸 평가할지 \n",
    "cocoEval = COCOeval(cocoGT,cocoDT,annType) #COCOeval객체 생성\n",
    "cocoEval.evaluate() #IoU를 계산하고, 평가를 수행하기 위한 준비\n",
    "cocoEval.accumulate() #precision, recall, AP 등을 계산\n",
    "cocoEval.summarize() #평가 결과를 요약하여 출력\n",
    "eval_stats = cocoEval.stats # 평가 결과를 변수에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuda 내의 메모리 해제\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "# 7. 동영상 예측 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '../../data/DRIVING-DATASET/sample_video.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(IMG_SIZE,IMG_SIZE)),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def model_predict(img, model):\n",
    "    img = transformer(img)\n",
    "    img = img.to(DEVICE)\n",
    "\n",
    "    prediction = model([img])\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid = cv2.VideoCapture(video_path)\n",
    "\n",
    "while(vid.isOpened):\n",
    "    ret, frame = vid.read()\n",
    "\n",
    "    if ret:\n",
    "        since = time()\n",
    "        original_h, original_w,_ = frame.shape\n",
    "    \n",
    "        prediction = model_predict(frame, model)\n",
    "        prediction = postprocess(prediction[0])\n",
    "        prediction[:,[0,2]] *= original_w/IMG_SIZE #박스의 크기를 프레임의 크기에 맞춰서 변환\n",
    "        prediction[:,[1,3]] *= original_h/IMG_SIZE\n",
    "\n",
    "        prediction[:,2].clip(min=0, max=original_w)\n",
    "        prediction[:,3].clip(min=0, max=original_h)\n",
    "\n",
    "        canvas = visualize(frame, prediction[:,0:4], prediction[:,5])\n",
    "        text = f'{(time()-since)*1000:.0f}ms/image'\n",
    "        cv2.putText(canvas, text, (20, 40), cv2.FONT_HERSHEY_PLAIN, 2, (255, 255, 255), 2)\n",
    "        cv2.imshow('camera', canvas)\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == 27:\n",
    "            break\n",
    "        if key == ord('s'):\n",
    "            cv2.waitKey()\n",
    "    \n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
